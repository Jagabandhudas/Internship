# -*- coding: utf-8 -*-
"""RAW GRU and CNN and Bi-LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AVv8NzshN4YG3Mxup-AumkT153-joXsu
"""

! pip install kaggle

# API to fetch dataset from kaggle
!kaggle datasets download -d kazanova/sentiment140

#Extract the file
from zipfile import ZipFile

dataset = '/content/sentiment140.zip'
with ZipFile(dataset, 'r') as zip:
    zip.extractall()
print('The dataset is extracted')

# Import necessary libraries
import numpy as np
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, GRU, LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, Bidirectional, concatenate
from tensorflow.keras.optimizers import Adam

# Download stopwords
import nltk
nltk.download('stopwords')
print(stopwords.words('english'))

# Read and preprocess the dataset

twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1')

twitter_data.shape



twitter_data.head()

column_names = ['target', 'id', 'date', 'flag', 'user', 'text']
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', names=column_names, encoding = 'ISO-8859-1')

twitter_data.shape

twitter_data.head()

# counting the number of missing value
twitter_data.isnull().sum()

twitter_data['target'].value_counts()

twitter_data.replace({'target':{4:1}}, inplace=True)

twitter_data['target'].value_counts()

# Define preprocessing functions
port_stem = PorterStemmer()
def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)
    stemmed_content = stemmed_content.lower()
    stemmed_connect = stemmed_content.split()
    stemmed_connect = [port_stem.stem(word) for word in stemmed_connect if not word in stopwords.words('english')]
    stemmed_connect = ' '.join(stemmed_connect)
    return stemmed_connect

twitter_data['stemmed_content'] = twitter_data['text'].apply(stemming)

twitter_data.head()

print(twitter_data['stemmed_content'])

print(twitter_data['target'])

x = twitter_data['stemmed_content'].values
y = twitter_data['target'].values

print(x)

print(y)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=2)

print(x.shape, x_train.shape, x_test.shape)

print(x_train)

print(x_test)

# Tokenize and pad sequences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(x_train)
x_train_seq = tokenizer.texts_to_sequences(x_train)
x_test_seq = tokenizer.texts_to_sequences(x_test)
vocab_size = len(tokenizer.word_index) + 1

max_length = max([len(seq) for seq in x_train_seq])
x_train_pad = pad_sequences(x_train_seq, maxlen=max_length, padding='post')
x_test_pad = pad_sequences(x_test_seq, maxlen=max_length, padding='post')

# Define the hybrid model
input_layer = Input(shape=(max_length,))

embedding_layer = Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length)(input_layer)

# GRU layer
gru_layer = Bidirectional(GRU(64, return_sequences=True))(embedding_layer)
gru_layer = GlobalMaxPooling1D()(gru_layer)

# CNN layer
cnn_layer = Conv1D(128, 5, activation='relu')(embedding_layer)
cnn_layer = MaxPooling1D(5)(cnn_layer)
cnn_layer = Conv1D(128, 5, activation='relu')(cnn_layer)
cnn_layer = GlobalMaxPooling1D()(cnn_layer)

# Bi-LSTM layer
lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)
lstm_layer = GlobalMaxPooling1D()(lstm_layer)

# Concatenate layers
concat_layer = concatenate([gru_layer, cnn_layer, lstm_layer])

# Fully connected layers
dense_layer = Dense(64, activation='relu')(concat_layer)
dropout_layer = Dropout(0.5)(dense_layer)
output_layer = Dense(1, activation='sigmoid')(dropout_layer)

# Compile the model
hybrid_model = Model(inputs=input_layer, outputs=output_layer)
hybrid_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
hybrid_model.summary()

# Train the model
hybrid_model.fit(x_train_pad, y_train, epochs=5, batch_size=64, validation_split=0.1)

# Evaluate the model
y_test_pred = (hybrid_model.predict(x_test_pad) > 0.5).astype("int32").flatten()
test_data_accuracy = accuracy_score(y_test, y_test_pred)
print('Accuracy score on the test data with hybrid model:', test_data_accuracy)